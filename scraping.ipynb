{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f031d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                     # importing for reading the file \n",
    "from bs4 import BeautifulSoup           # importing for data extraction\n",
    "from tqdm import tqdm                   # importing tqdm for processing\n",
    "import multiprocessing                  # importing for multiprocessing the data \n",
    "import requests                         # importing for http protocols or interactions with webserver\n",
    "import chardet                          # importing for knowing the encoding type of the text file \n",
    "import re                               # importing for removing uneccessary character in the text such as hyphens commas, fullstops\n",
    "import os                               # importing for setting file location\n",
    "\n",
    "import nltk                                                           # importing nltk library\n",
    "import nltk.data                                                      # going through data module  \n",
    "from nltk.tokenize import word_tokenize,pos_tag                       # importing the word tokenizer \n",
    "from nltk.corpus import cmudict                        \n",
    "from nltk.corpus import stopwords\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')     # loading the english pickle of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "490acf08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2345.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>432.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL\n",
       "0   123.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "1   321.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "2  2345.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "3  4321.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "4   432.0  https://insights.blackcoffer.com/rise-of-telem..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('Input.xlsx')        # reading the file \n",
    "df.head()                               # head of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f7c2dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(start,end):                                                       # function to data extraction\n",
    "    for i in tqdm(df.values[start:end]):                                           # looping\n",
    "        res      = requests.get(i[1])                                              # requesting for access the web\n",
    "        soup     = BeautifulSoup(res.content,'html.parser')                        # parsing through the content\n",
    "        try:                                                      \n",
    "            heading  = soup.title.text                                             # getting the title\n",
    "            if len(soup.find('div',class_ = 'td-post-content tagdiv-type').find_all('p')) > 10:  # check the len of p\n",
    "                contents = ''.join([t.text for t in soup.find('div',class_ = 'td-post-content tagdiv-type').find_all('p')]) # feeding to content variable\n",
    "            else:                                                                  # else\n",
    "                contents = soup2.find('div',class_ = 'td-post-content tagdiv-type').find('p').text + soup2.find('div',class_ = 'td-post-content tagdiv-type').find('ul').text # combining the ul tags and paragraph tags together to the content\n",
    "        except:\n",
    "            heading  = soup.title.text                                             # heading for exception \n",
    "            for j in range(len(soup.find_all('div',class_ = 'tdb-block-inner td-fix-index'))):  # loop through length of div class\n",
    "                if soup.find_all('div',class_ = 'tdb-block-inner td-fix-index')[j].find_all('p') != []: # check if the val not empty\n",
    "                    content = ''.join([t.text for t in soup.find_all('div',class_ = 'tdb-block-inner td-fix-index')[j].find_all('p')]) # feed to the content \n",
    "        \n",
    "        with open('Collections/'+str(i[0])+'.txt','w') as fd:                      # opening a folder called collection and writing the data onto the file \n",
    "            fd.write(heading)                                                      # headers collected\n",
    "            fd.write('\\n')                                                         # spacing \n",
    "            fd.write(contents)                                                     # contents collected \n",
    "            fd.close()                                                             # closing the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f28a5d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 57/57 [02:06<00:00,  2.22s/it]\n",
      "100%|███████████████████████████████████████████| 57/57 [02:12<00:00,  2.33s/it]\n"
     ]
    }
   ],
   "source": [
    "p1 = multiprocessing.Process(target = collect_data,args =(0,57))           # multiprocessing from 0 to 57 to the function\n",
    "p2 = multiprocessing.Process(target = collect_data,args =(57,114))         # multiprocessing from 57 to 114 to the function\n",
    "p1.start()                                                                 \n",
    "p2.start()                                                                 \n",
    "p1.join()\n",
    "p2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "848a95bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MasterDictionary/positive-words.txt','r') as fd2:            # opening file of positive words \n",
    "    text = fd2.read()                                                   # reading the file \n",
    "    fd2.close()                                                         # closing the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b9778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MasterDictionary/negative-words.txt', 'rb') as file:          # opening file of negative words as in readbin\n",
    "    result = chardet.detect(file.read())                                 # using chardet library to check the encoding of the file \n",
    "    \n",
    "with open('MasterDictionary/negative-words.txt', 'r', encoding=result['encoding']) as fd3: # opening againg the file of negative words \n",
    "    ntext = fd3.read()                                                                     # reading it     \n",
    "    fd3.close()                                                                            # closing it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e65011",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "###########################1.2.Creating a dictionary of Positive and Negative words#################################\n",
    "####################################################################################################################\n",
    "\n",
    "master_dict = {}                          # creating master dict\n",
    "\n",
    "positive = []                             # empty positive list \n",
    "negative = []                             # empty negative list\n",
    "for pos in text.split('\\n'):              # loop through positive text\n",
    "    positive.append(pos)                  # loading the value to positive list\n",
    "for neg in ntext.split('\\n'):             # loop through negative text\n",
    "    negative.append(neg)                  # loading the value to negative list\n",
    "    \n",
    "master_dict['positive'] = positive        # loading positive list to master dict\n",
    "master_dict['negative'] = negative        # loading negative list to master dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0b90c9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_analysis(start,stop):\n",
    "    def count_cleaned_words(text):\n",
    "        cleaned_word_count = len(words)\n",
    "        return cleaned_word_count\n",
    "\n",
    "    def count_syllables(word):\n",
    "        word = word.lower()\n",
    "        if word.endswith(\"es\") or word.endswith(\"ed\"):\n",
    "            return max(1, len(re.findall(r'[aeiouy]+', word)))\n",
    "        else:\n",
    "            return len(re.findall(r'[aeiouy]+', word))\n",
    "\n",
    "    def count_syllables_in_text(text):\n",
    "        words = re.findall(r'\\b\\w+\\b', text)                                   # Tokenize the text into words\n",
    "        syllable_counts = [count_syllables(word) for word in words]\n",
    "        return syllable_counts\n",
    "\n",
    "    def find_complex_words(text):\n",
    "        words = re.findall(r'\\b\\w+\\b', text)                                   # Tokenize the text into words\n",
    "        complex_words = [word for word in words if count_syllables(word) > 2]\n",
    "        return complex_words\n",
    "\n",
    "    # counting the total number of personal pronoun present in the paragraph or the data extracted\n",
    "    def count_pp(para):\n",
    "        personal_pronouns = ['I','we','you','he','she','it','they','my','mine','our','ours','your','yours','his','her',\n",
    "                         'hers','its','their','theirs','myself','yourself','himself','herself','itself','ourselves',\n",
    "                         'yourselves','themselves']\n",
    "        ls = []\n",
    "        for i in para.split(' '):\n",
    "            if i in personal_pronouns:\n",
    "                ls.append(i)\n",
    "        return len(ls)\n",
    "    \n",
    "    def calculate_average_word_length(text):\n",
    "        words = text.split()\n",
    "        total_char_count = sum(len(word) for word in words)\n",
    "        total_word_count = len(words)\n",
    "\n",
    "        if total_word_count == 0:\n",
    "            return 0  # Avoid division by zero\n",
    "\n",
    "        average_word_length = total_char_count / total_word_count\n",
    "        return average_word_length\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    data = []\n",
    "    for i in tqdm(df.values[start:stop]):                                                   # going through the values   \n",
    "        with open('Collections/' + str(i[0]) + '.txt','r') as fd:         # opening the text file \n",
    "            text = fd.read()                                              # reading the text file\n",
    "        # cleaning the text by removing the stopwords\n",
    "        custom_stopwords = set()\n",
    "        for filename in os.listdir(stopwords_folder):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(os.path.join(stopwords_folder, filename), \"r\",encoding = result['encoding']) as file:\n",
    "                    custom_stopwords.update(line.strip() for line in file)\n",
    "\n",
    "        words          = word_tokenize(text)                                              # Tokenize the paragraph into words\n",
    "        filtered_words = [word for word in words if word.lower() not in custom_stopwords] # Remove stopwords\n",
    "        clean_para     = \" \".join(filtered_words)                                         # Join the filtered words to reconstruct the paragraph\n",
    "        \n",
    "    #####################################################################################################################\n",
    "    ######################################1.3. Extracting Derived variables##############################################\n",
    "    #####################################################################################################################\n",
    "        tot_sent = len(tokenizer.tokenize(clean_para))                          # calculating the total number of sentences in the article \n",
    "        sent = tokenizer.tokenize(clean_para)                                   # sentences in the file \n",
    "        word_pattern = r\"[\\w'+,-]+\"                                             # word pattern \n",
    "        words = re.findall(word_pattern, clean_para)\n",
    "\n",
    "\n",
    "        # positive scoring \n",
    "        p = 0\n",
    "        n = 0\n",
    "        for words in text.split(' '):\n",
    "            if (words) in master_dict['positive']:\n",
    "                p += 1\n",
    "            elif (words) in master_dict['negative']:\n",
    "                n += 1\n",
    "        polarity_score = round((p-n)/((p+n)+0.000001),3)\n",
    "        subjective_score = round(((p + n)/((len(words))+0.000001)),3)\n",
    "        complex_words = len(find_complex_words(clean_para))\n",
    "        \n",
    "    #####################################################################################################################\n",
    "    ##########################################2. Analysis of Readability#################################################\n",
    "    #####################################################################################################################\n",
    "\n",
    "        average_sentence_length = round(len(words) / tot_sent,3)\n",
    "        try:\n",
    "            Per_Complex_words       = round((complex_words / len(words)*100),3)\n",
    "        except:\n",
    "            Per_Complex_words       = 0\n",
    "        fog_idx                 = round(0.4 * (average_sentence_length + Per_Complex_words),3)\n",
    "\n",
    "    #####################################################################################################################\n",
    "    #####################################3.Average Number of Words Per Sentence##########################################\n",
    "    #####################################################################################################################\n",
    "\n",
    "        average_word_length = round(len(words) / tot_sent,3)         # number of words per \n",
    "\n",
    "    #####################################################################################################################\n",
    "    ##########################################4.complex Word count#######################################################\n",
    "    #####################################################################################################################\n",
    "\n",
    "        complex_words\n",
    "\n",
    "    #####################################################################################################################\n",
    "    ###############################################5. Word count#########################################################\n",
    "    #####################################################################################################################\n",
    "\n",
    "        tot_word = count_cleaned_words(clean_para)\n",
    "\n",
    "    #####################################################################################################################\n",
    "    ##########################################6. Syllable Count Per Word#################################################\n",
    "    #####################################################################################################################\n",
    "\n",
    "        total_syllable = sum(count_syllables_in_text(text))\n",
    "\n",
    "    #####################################################################################################################\n",
    "    ###########################################7. personal pronoun#######################################################\n",
    "    #####################################################################################################################\n",
    "        \n",
    "        count_personal_pro = count_pp(text)\n",
    "        \n",
    "    #####################################################################################################################\n",
    "    ###########################################8.average word length#####################################################\n",
    "    #####################################################################################################################\n",
    "               \n",
    "        average_length = round(calculate_average_word_length(text),3)\n",
    "            \n",
    "    #####################################################################################################################\n",
    "        \n",
    "        data.append([i[0],i[1],p,n,polarity_score,subjective_score,average_sentence_length,Per_Complex_words,fog_idx,average_word_length,complex_words,tot_word,total_syllable,count_personal_pro,average_length])\n",
    "    final = pd.DataFrame(data,\n",
    "                         columns= ['URL_ID','URL','POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE',\n",
    "                                   'SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS',\n",
    "                                   'FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT',\n",
    "                                   'SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH'])    \n",
    "    \n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "73967fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 114/114 [00:11<00:00,  9.85it/s]\n"
     ]
    }
   ],
   "source": [
    "output = text_analysis(0,114)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6bedc336",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_excel('Final.xlsx',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
