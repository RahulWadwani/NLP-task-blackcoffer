{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f031d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                     # importing for reading the file \n",
    "from bs4 import BeautifulSoup           # importing for data extraction\n",
    "from tqdm import tqdm                   # importing tqdm for processing\n",
    "import multiprocessing                  # importing for multiprocessing the data \n",
    "import requests                         # importing for http protocols or interactions with webserver\n",
    "import chardet                          # importing for knowing the encoding type of the text file \n",
    "import re                               # importing for removing uneccessary character in the text such as hyphens commas, fullstops\n",
    "import os                               # importing for setting file location\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "490acf08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2345.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>432.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL\n",
       "0   123.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "1   321.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "2  2345.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "3  4321.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "4   432.0  https://insights.blackcoffer.com/rise-of-telem..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('Input.xlsx')        # reading the file \n",
    "df.head()                               # head of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f7c2dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(start,end):                                                       # function to data extraction\n",
    "    for i in tqdm(df.values[start:end]):                                           # looping\n",
    "        res      = requests.get(i[1])                                              # requesting for access the web\n",
    "        soup     = BeautifulSoup(res.content,'html.parser')                        # parsing through the content\n",
    "        try:                                                      \n",
    "            heading  = soup.title.text                                             # getting the title\n",
    "            if len(soup.find('div',class_ = 'td-post-content tagdiv-type').find_all('p')) > 10:  # check the len of p\n",
    "                contents = ''.join([t.text for t in soup.find('div',class_ = 'td-post-content tagdiv-type').find_all('p')]) # feeding to content variable\n",
    "            else:                                                                  # else\n",
    "                contents = soup2.find('div',class_ = 'td-post-content tagdiv-type').find('p').text + soup2.find('div',class_ = 'td-post-content tagdiv-type').find('ul').text # combining the ul tags and paragraph tags together to the content\n",
    "        except:\n",
    "            heading  = soup.title.text                                             # heading for exception \n",
    "            for j in range(len(soup.find_all('div',class_ = 'tdb-block-inner td-fix-index'))):  # loop through length of div class\n",
    "                if soup.find_all('div',class_ = 'tdb-block-inner td-fix-index')[j].find_all('p') != []: # check if the val not empty\n",
    "                    content = ''.join([t.text for t in soup.find_all('div',class_ = 'tdb-block-inner td-fix-index')[j].find_all('p')]) # feed to the content \n",
    "        \n",
    "        with open('Collections/'+str(i[0])+'.txt','w') as fd:                      # opening a folder called collection and writing the data onto the file \n",
    "            fd.write(heading)                                                      # headers collected\n",
    "            fd.write('\\n')                                                         # spacing \n",
    "            fd.write(contents)                                                     # contents collected \n",
    "            fd.close()                                                             # closing the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f28a5d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 57/57 [02:06<00:00,  2.22s/it]\n",
      "100%|███████████████████████████████████████████| 57/57 [02:12<00:00,  2.33s/it]\n"
     ]
    }
   ],
   "source": [
    "p1 = multiprocessing.Process(target = collect_data,args =(0,57))           # multiprocessing from 0 to 57 to the function\n",
    "p2 = multiprocessing.Process(target = collect_data,args =(57,114))         # multiprocessing from 57 to 114 to the function\n",
    "p1.start()                                                                 \n",
    "p2.start()                                                                 \n",
    "p1.join()\n",
    "p2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e29bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4639ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c2cd15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f048e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61fdad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d50a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f6cbd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f9eeaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "848a95bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MasterDictionary/positive-words.txt','r') as fd2:            # opening file of positive words \n",
    "    text = fd2.read()                                                   # reading the file \n",
    "    fd2.close()                                                         # closing the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b9778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MasterDictionary/negative-words.txt', 'rb') as file:          # opening file of negative words as in readbin\n",
    "    result = chardet.detect(file.read())                                 # using chardet library to check the encoding of the file \n",
    "    \n",
    "with open('MasterDictionary/negative-words.txt', 'r', encoding=result['encoding']) as fd3: # opening againg the file of negative words \n",
    "    ntext = fd3.read()                                                                     # reading it     \n",
    "    fd3.close()                                                                            # closing it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e65011",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "###########################1.2.Creating a dictionary of Positive and Negative words#################################\n",
    "####################################################################################################################\n",
    "\n",
    "master_dict = {}                          # creating master dict\n",
    "\n",
    "positive = []                             # empty positive list \n",
    "negative = []                             # empty negative list\n",
    "for pos in text.split('\\n'):              # loop through positive text\n",
    "    positive.append(pos)                  # loading the value to positive list\n",
    "for neg in ntext.split('\\n'):             # loop through negative text\n",
    "    negative.append(neg)                  # loading the value to negative list\n",
    "    \n",
    "master_dict['positive'] = positive        # loading positive list to master dict\n",
    "master_dict['negative'] = negative        # loading negative list to master dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7a1f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                                                           # importing nltk library\n",
    "import nltk.data                                                      # going through data module  \n",
    "from nltk.tokenize import word_tokenize                               # importing the word tokenizer \n",
    "from nltk.corpus import cmudict                        \n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')     # loading the english pickle of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0b90c9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.475 0.0 6.19\n"
     ]
    }
   ],
   "source": [
    "# Create a function to count the number of syllables in a word\n",
    "def count_syllables(word, pronunciations):\n",
    "    return max(len(list(filter(str.isdigit, s))) for s in pronunciations.get(word.lower(), [[]]))\n",
    "\n",
    "stopwords_folder = \"/home/rahul/coding/blackcoffer internship project work/StopWords\"  # setting the stopwords path\n",
    "\n",
    "for i in df.values:                                                   # going through the values   \n",
    "    with open('Collections/' + str(i[0]) + '.txt','r') as fd:         # opening the text file \n",
    "        text = fd.read()                                              # reading the text file\n",
    "    # cleaning the text by removing the stopwords\n",
    "    custom_stopwords = set()\n",
    "    for filename in os.listdir(stopwords_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(stopwords_folder, filename), \"r\",encoding = result['encoding']) as file:\n",
    "                custom_stopwords.update(line.strip() for line in file)\n",
    "\n",
    "    words          = word_tokenize(text)                                              # Tokenize the paragraph into words\n",
    "    filtered_words = [word for word in words if word.lower() not in custom_stopwords] # Remove stopwords\n",
    "    clean_para     = \" \".join(filtered_words)                                         # Join the filtered words to reconstruct the paragraph\n",
    "    \n",
    "#####################################################################################################################\n",
    "######################################1.3. Extracting Derived variables##############################################\n",
    "#####################################################################################################################\n",
    "    tot_sent = len(tokenizer.tokenize(clean_para))                          # calculating the total number of sentences in the article \n",
    "    sent = tokenizer.tokenize(clean_para)                                   # sentences in the file \n",
    "    word_pattern = r\"[\\w'+,-]+\"                                             # word pattern \n",
    "    words = re.findall(word_pattern, clean_para)\n",
    "    tot_word = len(words)\n",
    "    \n",
    "    # positive scoring \n",
    "    p = 0\n",
    "    n = 0\n",
    "    for words in clean_para.split(' '):\n",
    "        if (words) in master_dict['positive']:\n",
    "            p += 1\n",
    "        elif (words) in master_dict['negative']:\n",
    "            n += 1\n",
    "            \n",
    "    polarity_score = round((p-n)/((p+n)+0.000001),3)\n",
    "    subjective_score = round(((p + n)/((after_clean)+0.000001)),3)\n",
    "    \n",
    "    # Get the CMU Pronouncing Dictionary\n",
    "    pronunciations = cmudict.dict()\n",
    "\n",
    "    # Define a threshold for the number of syllables to consider a word complex\n",
    "    complexity_threshold = 2 # You can adjust this threshold based on your definition of complexity\n",
    "\n",
    "    # Find complex words in the paragraph\n",
    "    complex_words = [word for word in words if count_syllables(word, pronunciations) >= complexity_threshold]\n",
    "    \n",
    "#####################################################################################################################\n",
    "##########################################2. Analysis of Readability#################################################\n",
    "#####################################################################################################################\n",
    "\n",
    "    average_sentence_length = round(tot_word / tot_sent,3)\n",
    "    Per_Complex_words       = round((len(complex_words) / tot_word)*100,3)\n",
    "    fog_idx                 = 0.4 * (average_sentence_length + Per_Complex_words)\n",
    "\n",
    "#####################################################################################################################\n",
    "#####################################3.Average Number of Words Per Sentence##########################################\n",
    "#####################################################################################################################\n",
    "    average_sentence_length = round(tot_word / tot_sent,3)\n",
    "\n",
    "\n",
    "#     print(average_sentence_length,Per_Complex_words,fog_idx)\n",
    "#     print(p,n,polarity_score,subjective_score,len(complex_words))   \n",
    "#     print(f' Total Words : {tot_word}\\n Total sentences : {tot_sent}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b196db61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(944, 61, 15.475409836065573)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_word,tot_sent,tot_word/tot_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb806e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
